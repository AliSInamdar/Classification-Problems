# -*- coding: utf-8 -*-
"""Project2_Ali_Inamdar_IntroToDS (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Oo-YQtK2IabcEoxopYVt9HhjdJNpXWFY

## Ali Inamdar

## Project 2 - Classification Algorithms

## Introduction to DataScience.

Impoorting all libraries required for this project.
"""

!pip install xgboost
!pip install catboost

#Data Loading and Manilupation Libraries
import numpy as np
import pandas as pd
#Data Visualization Libraries
import matplotlib.pyplot as plt
import plotly.offline as py
import seaborn as sns
from plotly import graph_objs as go
#Model Training and Testing Library
from sklearn.model_selection import train_test_split
from sklearn.model_selection import  GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
#Machine Learning Model Libraries
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
import xgboost as xgb
from xgboost import XGBClassifier
#Machine Learning Model Evaluation Libraries
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.datasets import make_classification

"""## Data Pre-processing, Cleaning and Wrangling.

Importing the Dataset.
"""

data1 = pd.read_csv('/content/project2_data.csv')
data1

data1.columns

#Getting information about the dataset.
data1.info()

#Checking for null values throughout the datasaet and printing them out.
data1.isnull().sum()

"""Splitting the dataset into 'x' and 'y'."""

#Converting the target variable column into 'int' datatype.
data1['y'] = data1['y'].map({'yes': 1, 'no': 0})
data1['y'] = data1['y'].astype(int)

X = data1[['age','campaign','pdays','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed']]
y = data1['y']

X

y

"""## Models & Machine Learning Classifiers

Train , Test and Split
"""

#Using the train, test and split method to provide the suitable inputs for our machinne learning models.
#Splitting the dataset into a 70/30 test-train ratio.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""## 1. Random Forest Classifier"""

#Declaring the model
rf_classifier = RandomForestClassifier()

#Fitting/Training our model with the Train dataset.
rf_classifier.fit(X_train, y_train)

#Predicting
rf_predictions = rf_classifier.predict(X_test)

#Calculating the accuracy of the Random Forest Model.
rf_accuracy = accuracy_score(y_test, rf_predictions)

#Confusion Matrix for the Random Forest Model.
rf_conf_matrix = confusion_matrix(y_test, rf_predictions)

#Printing the Confusion Matrix
rf_conf_matrix

#Plotting the Confusion Matrix Heatmap.
plt.figure(figsize=(5, 4))
sns.heatmap(rf_conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Logistic Regression - Confusion Matrix Heatmap")
plt.show()

#Printing The Scores
print("Random Forest Classifier:")
print(classification_report(y_test, rf_predictions))

#Computing the  ROC curve and AUC for Random Forest Classifier
fpr, tpr, thresholds = roc_curve(y_test, rf_predictions)
rf_roc_auc = auc(fpr, tpr)

#Printing The ROC Curve Value
rf_roc_auc

#Plotting the ROC curve for the Random Forest Classifier
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {rf_roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Line of no-discrimination
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""## 2. Decision Tree Classifier"""

#Using Decision Tree Classifier
dt_classifier = DecisionTreeClassifier()

#Fitting the Decision Tree Classifier with our training dataset.
dt_classifier.fit(X_train, y_train)

#Making Predictions
dt_predictions = dt_classifier.predict(X_test)

#Calculating the accuracy for Decision Tree Classifier.
dt_accuracy = accuracy_score(y_test, dt_predictions)

#Building the Confusion matrix for Decision Tree Classifier results.
dt_conf_matrix = confusion_matrix(y_test, dt_predictions)

#Printing the confusuion matrix.
print("\nDecision Tree Classifier Confusion Matrix:")
dt_conf_matrix

#Plotting the Confusion Matrix Heatmap.
plt.figure(figsize=(5, 4))
sns.heatmap(dt_conf_matrix, annot=True, fmt="d", cmap="Reds", xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Decision Tree - Confusion Matrix Heatmap")
plt.show()

#Classification report for Decisio Tree Classifier.
print("\nDecision Tree Classifier:")
print(classification_report(y_test, dt_predictions))

#Computing the ROC curve and AUC for Decision Tree Classifier
fpr, tpr, thresholds = roc_curve(y_test, dt_predictions)
dt_roc_auc = auc(fpr, tpr)

#Printing The ROC Curve Value
dt_roc_auc

#Plotting the ROC curve for the Decision Tree Classifier
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='red', label=f'ROC Curve (AUC = {dt_roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Line of no-discrimination
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""## 3. K-Nearest Neighbors(KNN) Classifier"""

#Using KNN Classifier.
knn_classifier = KNeighborsClassifier()

#Fitting/Training the model with our training dataset.
knn_classifier.fit(X_train, y_train)

#Making Predictions
knn_predictions = knn_classifier.predict(X_test)

#Calculating the accuracy of KNN Classifier.
knn_accuracy = accuracy_score(y_test, knn_predictions)

#Building the Confusion matrix for KNN Classifier.
knn_conf_matrix = confusion_matrix(y_test, knn_predictions)

#Printing the Confusion matrix.
print("\nKNN Classifier Confusion Matrix:")
knn_conf_matrix

#Heatmap for KNN Classifier.
plt.figure(figsize=(5, 4))
sns.heatmap(knn_conf_matrix, annot=True, fmt="d", cmap="Greens", xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("KNN Classifier - Confusion Matrix Heatmap")
plt.show()

#Classification report for KNN Classifier.
print("\nKNN Classifier:")
print(classification_report(y_test, knn_predictions))

#Computing the ROC curve and AUC for KNN Classifier
fpr, tpr, thresholds = roc_curve(y_test, knn_predictions)
knn_roc_auc = auc(fpr, tpr)

#Printing The ROC Curve Value
knn_roc_auc

#Plotting the ROC curve for the KNN Classifier
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='green', label=f'ROC Curve (AUC = {knn_roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Line of no-discrimination
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""## 4. Logistic Regression Classifier"""

#Using Logistic Regression Classifier
lr_classifier = LogisticRegression()

#Fitting the model with our tarining dataset
lr_classifier.fit(X_train, y_train)

#Predicting
lr_predictions = lr_classifier.predict(X_test)

#Calculating the accuracy of the Logistic Regression Model.
lr_accuracy = accuracy_score(y_test, lr_predictions)

#Confusion Matrix for the  Logistic Regression Model.
lr_conf_matrix = confusion_matrix(y_test, lr_predictions)

#Printing the Confusion Matrix
lr_conf_matrix

#Plotting the Confusion Matrix Heatmap.
plt.figure(figsize=(5, 4))
sns.heatmap(lr_conf_matrix, annot=True, fmt="d", cmap="Oranges", xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Logistic Regression - Confusion Matrix Heatmap")
plt.show()

#Printing The Scores
print("Logistic Regression Classifier:")
print(classification_report(y_test, lr_predictions))

#Computing the ROC curve and AUC for Logistic Regression Classifier
fpr, tpr, thresholds = roc_curve(y_test, lr_predictions)
lr_roc_auc = auc(fpr, tpr)

#Printing The ROC Curve Value
lr_roc_auc

#Plotting the ROC curve for the Logistic Regression Classifier
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='orange', label=f'ROC Curve (AUC = {lr_roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Line of no-discrimination
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""## 5. Naive Baye's Classifier"""

#Using Navie Baye's Classifier
nb_classifier = GaussianNB()

#Fitting the model with our training dataset
nb_classifier.fit(X_train, y_train)

#Predicting
nb_predictions = nb_classifier.predict(X_test)

#Calculating the accuracy of the Naive Baye's Model.
nb_accuracy = accuracy_score(y_test, nb_predictions)

#Confusion Matrix for the Naive Baye's Model.
nb_conf_matrix = confusion_matrix(y_test, nb_predictions)

#Printing the Confusion Matrix
nb_conf_matrix

#Plotting the Confusion Matrix Heatmap.
plt.figure(figsize=(5, 4))
sns.heatmap(nb_conf_matrix, annot=True, fmt="d", cmap="Purples", xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Naive Baye's - Confusion Matrix Heatmap")
plt.show()

#Printing The Scores
print("Naive Baye's Classifier:")
print(classification_report(y_test, nb_predictions))

#Computing the ROC curve and AUC for Naive Baye's Classifier
fpr, tpr, thresholds = roc_curve(y_test, nb_predictions)
nb_roc_auc = auc(fpr, tpr)

#Printing The ROC Curve Value
nb_roc_auc

#Plotting the ROC curve for the Naive Baye's Classifier
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='purple', label=f'ROC Curve (AUC = {nb_roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Line of no-discrimination
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""## 6. XGBoost Algorithm Classifier"""

#Using XGBoost Algorithm
xgb_classifier = XGBClassifier()

#Fitting the model with our training dataset
xgb_classifier.fit(X_train, y_train)

#Predicting
xgb_predictions = xgb_classifier.predict(X_test)

#Calculating the accuracy of XGBost Algorithm
xgb_accuracy = accuracy_score(y_test, xgb_predictions)

#Confusion Matrix for the XGBoost Algorithm Model.
xgb_conf_matrix = confusion_matrix(y_test, xgb_predictions)

#Printing the Confusion Matrix
xgb_conf_matrix

#Plotting the Confusion Matrix Heatmap.
plt.figure(figsize=(5, 4))
sns.heatmap(xgb_conf_matrix, annot=True, fmt="d", cmap="coolwarm", xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("XGBoost Algorithm - Confusion Matrix Heatmap")
plt.show()

#Printing The Scores
print("XGBoost Algorithm Classifier:")
print(classification_report(y_test, xgb_predictions))

#Computing the ROC curve and AUC for Naive Baye's Classifier
fpr, tpr, thresholds = roc_curve(y_test, xgb_predictions)
xgb_roc_auc = auc(fpr, tpr)

#Printing The ROC Curve Value
xgb_roc_auc

#Plotting the ROC curve for the Naive Baye's Classifier
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='cyan', label=f'ROC Curve (AUC = {xgb_roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Line of no-discrimination
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""## Comparing the Accuracy of all the Machine Learning Models used:"""

#Comparing Classifier Accuracies
print("\nComparison of Classifiers:")
print(f"Random Forest Classifier Accuracy: {rf_accuracy}")
print(f"\nDecision Tree Classifier Accuracy: {dt_accuracy}")
print(f"\nKNN Classifier Accuracy: {knn_accuracy}")
print(f"\nLogistic Regression Classifier Accuracy: {lr_accuracy}")
print(f"\nNaive Baye's Classifier Accuracy: {nb_accuracy}")
print(f"\nXGBoost Algorithm Classifier Accuracy: {xgb_accuracy}")

"""## ReRunning the Random Forest Classifier with HyperParameter Tuning."""

#Creating a Random Forest Classifier model
rf = RandomForestClassifier(random_state=42)

param_grid = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Randomized search for tuning our model
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid,
                                   n_iter=50,  # Number of parameter settings sampled
                                   scoring='accuracy',
                                   n_jobs=-1,
                                   cv=3,
                                   verbose=2,
                                   random_state=42)

#Fitting our model with hyperparameter tuning
random_search.fit(X_train, y_train)

#Evaluating the model
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

#Printing the list of best parameters
print("Best Parameters Found:\n", random_search.best_params_)

#Printing the accuracy of out hyperparameter ttuned model
print("Model Accuracy on Test Set:", accuracy)

#Making the confusion matrix for our model
hyper_conf_matrix = confusion_matrix(y_test, y_pred)

#Printing the confusion matrix
hyper_conf_matrix

#Plotting the Confusion Matrix Heatmap.
plt.figure(figsize=(5, 4))
sns.heatmap(hyper_conf_matrix, annot=True, fmt="d", cmap="magma", xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("HyperParameter Tuned Random Forest Classifier - Confusion Matrix Heatmap")
plt.show()

#Printing The Scores
print("HyperParameter Tuned Random Forest Classifier:")
print(classification_report(y_test, y_pred))

#Computing the ROC curve and AUC for HyperParameter Tuned Random Forest Classifier
fpr, tpr, thresholds = roc_curve(y_test, y_pred)
hyper_roc_auc = auc(fpr, tpr)

#Printing The ROC Curve Value
hyper_roc_auc

#Plotting the ROC curve for the Naive Baye's Classifier
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='magenta', label=f'ROC Curve (AUC = {hyper_roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Line of no-discrimination
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""Adding **CATBOOST ALGRORITHM CLASSIFIER**



"""

#Importing the CatBoost CLassifier
from catboost import CatBoostClassifier ,Pool

#Splitting the dataset into x and y
x = data1.drop(columns=['y'])
y = data1['y']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

#Identifying categorical features
categorical_features_indices = [X.columns.get_loc(col) for col in ['job', 'marital', 'education', 'default', 'housing', 'loan','contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays','previous', 'poutcome', 'emp.var.rate', 'cons.price.idx','cons.conf.idx', 'euribor3m', 'nr.employed']]

categorical_features_names = ['job', 'marital', 'education', 'default', 'housing', 'loan','contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays','previous', 'poutcome', 'emp.var.rate', 'cons.price.idx','cons.conf.idx', 'euribor3m', 'nr.employed']
for col in categorical_features_names:
    if x[col].dtype == float:
        x[col] = x[col].astype(int).astype(str) # Convert float to int then to string if needed
    elif x[col].dtype != object and x[col].dtype != int:
        x[col] = x[col].astype(str) # Convert other types to string
#Recalculate categorical_features_indices after converting data types
categorical_features_indices = [x.columns.get_loc(col) for col in categorical_features_names]

#Defining the CatBoost model
model = CatBoostClassifier(
    iterations=500,
    learning_rate=0.1,
    depth=10,
    verbose=100
)

#Fitting the model
model.fit(x_train, y_train, cat_features=categorical_features_indices)

y_predict = model.predict(x_test)

accuracy = accuracy_score(y_test, y_predict)
print("Accuracy:", accuracy)

catboost_conf_matrix = confusion_matrix(y_test, y_predict)

catboost_conf_matrix

#Plotting the Confusion Matrix Heatmap.
plt.figure(figsize=(5, 4))
sns.heatmap(catboost_conf_matrix, annot=True, fmt="d", cmap="coolwarm", xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("XGBoost Algorithm - Confusion Matrix Heatmap")
plt.show()

#Printing The Scores
print("CatBoost Classifier:")
print(classification_report(y_test, y_predict))

#Computing the ROC curve and AUC for CatBoost Classifier
fpr, tpr, thresholds = roc_curve(y_test, y_predict)
catboost_roc_auc = auc(fpr, tpr)

catboost_roc_auc

#Plotting the ROC curve for the CatBoost Classifier
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='magenta', label=f'ROC Curve (AUC = {catboost_roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Line of no-discrimination
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""## Conclusion:
Accuracy:

Logistic Regression Classifier had the highest accuracy of all the models used at 90% , only seconded by the XGBoost Algorithm Classifier at 89%.
All models had a accuracy of over 88% , apart from the Naive Baye’s Classifier which had a accuracy of 87%.
The Accuracy can be improved for all models if we makes change in the train/test split ratio.
The accuracy of the HyperParamter Tuned Random Forest Classifier is 90% ,which is an improvement over the  the regular Random Forest Classifier that we have used earlier.





ROC Curve & AUC:

Naive Baye’s Classifier had the highest ROC & AUC value of all the models used at 0.72, closely followed by Random Forest and KNN Classifiers at 0.62.
All models had a score over 0.60 , apart from the Logistic Regression Classifier which had a score of just 0.59.
The ROC & AUC score of the HyperParamter Tuned Random Forest Classifier is 0.59 ,which is a dip in performance over the  the regular Random Forest Classifier that we have used earlier.
"""